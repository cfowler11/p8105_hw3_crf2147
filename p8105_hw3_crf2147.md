Data Science I Homework 3
================
Charlotte Fowler
10/9/2019

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.2.1     ✔ purrr   0.3.2
    ## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
    ## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
    ## ✔ readr   1.3.1     ✔ forcats 0.4.0

    ## ── Conflicts ────────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
```

# Problem 1

``` r
data("instacart")
```

The goal is to do some exploration of this dataset. To that end, write a
short description of the dataset, noting the size and structure of the
data, describing some key variables, and giving illstrative examples of
observations. Then, do or answer the following (commenting on the
results of each):

This dataset has 1384617 rows and 15 variables. Each row represents a
different item ordered by a instacart customer. We have information on
the order, such as `order_id`, `order_hour_of_day`, `order_dow` or the
day of week in which the order occured, and `add_to_cart_order` which
shows the order in which items were added to the cart. We also have
information on the customer such as `user_id`, `order_number` or the
number of prior orders (+1) this customer has had, and
`days_since_prior_order`. Lastly we have information on the product
being requested such as `product_id`, `product_name`, `aisle_id`,
`aisle`, `department_id`, and `department`.

For example, we know that customer 127134, in their 10th order from
instacart, on a Friday at 9am added ‘Hash Brown Potato Patties’, in
frozen appetizers sides aisle from the frozen department, for the first
time as the 21st item in their cart. We can also see that 19 other
orders in the data contained ‘Hash Brown Potato Patties’.

``` r
instacart %>% 
  filter(product_name == 'Hash Brown Potato Patties') %>% 
  nrow()
```

How many aisles are there, and which aisles are the most items ordered
from?

``` r
instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  nrow() 
```

    ## [1] 134

``` r
instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  nrow()
```

    ## [1] 134

``` r
instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  arrange(-n) %>% 
  filter(n>70000)
```

    ## # A tibble: 3 x 2
    ## # Groups:   aisle [3]
    ##   aisle                           n
    ##   <chr>                       <int>
    ## 1 fresh vegetables           150609
    ## 2 fresh fruits               150473
    ## 3 packaged vegetables fruits  78493

There are 134 aisles. The most popular are fresh vegetables, fresh
fruits, and packaged vegetables fruits.

Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it.

``` r
instacart %>% 
  group_by(aisle) %>% 
  count() %>% 
  filter(n>10000) %>% 
  inner_join (instacart, by = 'aisle') %>% 
  select(aisle, n, department) %>% 
  distinct() %>% 
  arrange(-n) %>% 
  ggplot(aes(x = reorder(aisle, -n), y = n, fill = department)) +
  geom_bar(stat = "identity") +
  coord_flip()
```

![](p8105_hw3_crf2147_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables
fruits”. Include the number of times each item is ordered in your
table.

``` r
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits" )) %>% 
  select(product_name, aisle) %>%   
  group_by(aisle, product_name) %>% 
  count()
```

    ## # A tibble: 1,114 x 3
    ## # Groups:   aisle, product_name [1,114]
    ##    aisle              product_name                                        n
    ##    <chr>              <chr>                                           <int>
    ##  1 baking ingredients 1 to 1 Gluten Free Baking Flour                     5
    ##  2 baking ingredients 1-to-1 Baking Flour, Gluten/Wheat/Dairy Free        2
    ##  3 baking ingredients 100% Cacao Unsweetened Chocolate Baking Bar         3
    ##  4 baking ingredients 100% Natural Sweetener Zero Calorie Packets        16
    ##  5 baking ingredients 100% Natural Zero Calorie Sweetener                 2
    ##  6 baking ingredients 100% Organic Einkorn  All-Purpose Flour             2
    ##  7 baking ingredients 100% Organic Premium Whole Wheat Flour              2
    ##  8 baking ingredients 100% Organic Unbleached All-Purpose Flour           8
    ##  9 baking ingredients 100% Organic Unbleached White Whole Wheat Flour     1
    ## 10 baking ingredients 100% Pure Corn Starch                              22
    ## # … with 1,104 more rows

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week; format this
table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>% 
  filter(product_name == c('Pink Lady Apples', 'Coffee Ice Cream')) %>% 
  select(order_hour_of_day, order_dow, product_name) %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean(order_hour_of_day)) %>% 
  pivot_wider(names_from = product_name, values_from = 'mean(order_hour_of_day)') %>% 
  ungroup() %>% 
  rename(Day = order_dow) %>% 
  mutate(
    Day = recode(Day, '0' = "Monday", '1' = "Tuesday", '2'="Wednesday", '3'="Thursday", '4'="Friday", '5'="Saturday", '6'="Sunday"), 
  ) %>% 
  knitr::kable()
```

    ## Warning in product_name == c("Pink Lady Apples", "Coffee Ice Cream"):
    ## longer object length is not a multiple of shorter object length

| Day         |    Coffee Ice Cream |                Pink Lady Apples |
| :---------- | ------------------: | ------------------------------: |
| Monday      |            13.22222 |                        12.25000 |
| Tuesday     |            15.00000 |                        11.67857 |
| Wednesday   |            15.33333 |                        12.00000 |
| Thursday    |            15.40000 |                        13.93750 |
| Friday      |            15.16667 |                        11.90909 |
| Saturday    |            10.33333 |                        13.86957 |
| Sunday      |            12.35294 |                        11.55556 |
| ’Coffee Ice | Cream: Mean Hour Or |    dered’ = ‘Coffee Ice Cream’, |
| ’Pink La    | dy Apples: Mean Hou | r Ordered’ = ‘Pink Lady Apples’ |

# Problem 2

``` r
data("brfss_smart2010")
```

First, do some data cleaning:

format the data to use appropriate variable names; focus on the “Overall
Health” topic include only responses from “Excellent” to “Poor” organize
responses as a factor taking levels ordered from “Poor” to “Excellent”

``` r
brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  separate(locationdesc, 
           into = c('state', 'county'),
           sep = " - ") %>% 
  select(-locationabbr) %>% 
  filter(
    topic == 'Overall Health',
    response == c('Excellent', 'Good', 'Very Good', 'Fair', 'Poor')
  ) %>% 
  mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very Good", "Excellent")))
```

    ## Warning in response == c("Excellent", "Good", "Very Good", "Fair", "Poor"):
    ## longer object length is not a multiple of shorter object length

    ## Warning: Unknown levels in `f`: Very Good

In 2002, which states were observed at 7 or more locations? What about
in 2010?

``` r
brfss_smart2010 %>% 
  filter(year==2002) %>% 
  count(state) %>% 
  filter(n>=7)
```

    ## # A tibble: 3 x 2
    ##   state     n
    ##   <chr> <int>
    ## 1 MA        9
    ## 2 NJ       10
    ## 3 PA       15

``` r
brfss_smart2010 %>% 
  filter(year == 2010) %>% 
  count(state) %>% 
  filter(n>=7)
```

    ## # A tibble: 8 x 2
    ##   state     n
    ##   <chr> <int>
    ## 1 CA       13
    ## 2 FL       35
    ## 3 ID        7
    ## 4 MA        8
    ## 5 MD       12
    ## 6 NY       11
    ## 7 PA        8
    ## 8 TX       14

In 2002, Massachusetts, New Jersey and Pennsylvania were observed at 7
or more locations. In 2010, California, Florida, Idaho, Massachusetts,
Maryland, New York, Pennsylvania, and Texas were observed in 7 or more
locations.

Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data\_value
across locations within a state. Make a “spaghetti” plot of this average
value over time within a state (that is, make a plot showing a line for
each state across years – the geom\_line geometry and group aesthetic
will help).

``` r
brfss_smart2010 %>% 
  filter(response == 'Excellent') %>% 
  group_by(state, year) %>% 
  summarize(mean_value = mean(data_value)) %>% 
  ggplot(
    aes(x = year, y = mean_value, color = state)) +
  geom_line() + 
  labs(x = 'Year', y = 'Mean Data Value') + 
  viridis::scale_color_viridis(
    discrete = TRUE, 
    name = "State"
    ) +
  theme_bw() 
```

    ## Warning: Removed 2 rows containing missing values (geom_path).

![](p8105_hw3_crf2147_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

``` r
brfss_smart2010 %>% 
  filter(
    year %in% c(2006, 2010), 
    state == 'NY'
  ) %>% 
  ggplot(aes( x = response, y = data_value)) + 
  geom_bar(stat = "identity") + 
  facet_grid(~year)
```

![](p8105_hw3_crf2147_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->

# Problem 3

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes. Describe the resulting dataset
(e.g. what variables exist, how many observations, etc).

``` r
accel_data = read_csv("./data/accel_data.csv")
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
accel_data = accel_data %>% 
  pivot_longer(
    'activity.1':'activity.1440',
    values_to = 'activity', 
    names_to = 'minute'
  ) %>% 
  separate(minute, 
           into = c('non', 'minute'),
           sep = 9) %>% 
  select(-non) %>% 
  mutate(
    weekend = ifelse(day == c("Sunday", "Saturday"), TRUE, FALSE ),
    minute = as.numeric(minute),
    day = factor(day),
    week = factor(week)
  )
```

The dataset `accel_data` consists of 50400 observations by 6 variables.
Each observation represents a minute of data observed by the
accelerometer. We have information on the `minute`, `day`, and `week`,
as well as a logical variable on whether it was the `weekend`. We also
have information on `day.id`, or how many days the patient has been
wearing the device. Lastly, we have the `activity` for that minute.

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes. Describe the resulting dataset
(e.g. what variables exist, how many observations, etc).

Traditional analyses of accelerometer data focus on the total activity
over the day. Using your tidied dataset, aggregate accross minutes to
create a total activity variable for each day, and create a table
showing these totals. Are any trends apparent?

Accelerometer data allows the inspection activity over the course of the
day. Make a single-panel plot that shows the 24-hour activity time
courses for each day and use color to indicate day of the week. Describe
in words any patterns or conclusions you can make based on this graph.
